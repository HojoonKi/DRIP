import os, math, random, argparse, torch, torch.nn.functional as F
import glob
import json
import csv
import copy
from datetime import datetime
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from torchvision import transforms
from tqdm import tqdm
from tqdm import tqdm
from diffusers import (
    StableDiffusionImg2ImgPipeline,
    AutoencoderKL,
    DDPMScheduler,
    UNet2DConditionModel,
)
from diffusers.loaders import AttnProcsLayers
from peft import LoraConfig, get_peft_model
from transformers import CLIPProcessor, CLIPModel
import torchreid
from accelerate import Accelerator
from facenet_pytorch import MTCNN
import numpy as np
import copy

# ----------------------------
# 1) í•˜ì´í¼íŒŒë¼ë¯¸í„° & ì¸ì
# ----------------------------
def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--mode", type=str, default="train", choices=["train", "inference"],
                   help="ì‹¤í–‰ ëª¨ë“œ: train ë˜ëŠ” inference")
    p.add_argument("--data_root",   type=str, default="dataset")
    p.add_argument("--pretrained_model", type=str,
                   default="runwayml/stable-diffusion-v1-5")
    p.add_argument("--resolution", type=int, default=512)
    p.add_argument("--batch_size", type=int, default=2)
    p.add_argument("--lr",         type=float, default=1e-4)
    p.add_argument("--epochs",     type=int, default=3)
    p.add_argument("--l_face",     type=float, default=1.0)   # Î»â‚
    p.add_argument("--l_text",     type=float, default=1.0)   # Î»â‚‚
    p.add_argument("--l_kl",       type=float, default=0.1)   # Î»â‚ƒ, KL divergence weight
    p.add_argument("--save_dir",   type=str, default="lora_out")
    
    # Inference ê´€ë ¨ ì˜µì…˜ë“¤
    p.add_argument("--lora_path", type=str, default=None,
                   help="inferenceì‹œ ë¡œë“œí•  LoRA ê°€ì¤‘ì¹˜ ê²½ë¡œ")
    p.add_argument("--output_dir", type=str, default="inference_output",
                   help="inference ê²°ê³¼ ì €ì¥ ê²½ë¡œ")
    p.add_argument("--num_inference_steps", type=int, default=50,
                   help="inferenceì‹œ diffusion steps ìˆ˜")
    p.add_argument("--guidance_scale", type=float, default=7.5,
                   help="classifier-free guidance scale")
    p.add_argument("--strength", type=float, default=0.8,
                   help="img2img strength (0.0-1.0)")
    
    # ë°ì´í„°ì…‹ ê´€ë ¨ ì˜µì…˜ë“¤
    p.add_argument("--celeba_dir", type=str,
               default="dataset/celeba-hq/celeba-256",
               help="CelebA-HQ 256px ì–¼êµ´ ì´ë¯¸ì§€ í´ë”")
    p.add_argument("--mpii_dir", type=str, 
               default="dataset/mpii-one-person",
               help="MPII ë°ì´í„°ì…‹ ë£¨íŠ¸ ë””ë ‰í† ë¦¬")
    
    return p.parse_args()

# ----------------------------
# 2) ë°ì´í„°ì…‹
# ----------------------------

class MixedFacePoseDataset(Dataset):
    """
    CelebA ì–¼êµ´ â†” MPII ì‚¬ëŒ ì´ë¯¸ì§€Â·ìº¡ì…˜ì„ ë§¤ ìŠ¤í… ëœë¤ ë§¤ì¹­
    ë°˜í™˜: ref(ì–¼êµ´), orig(ì‚¬ëŒ+ë°°ê²½), prompt(ìº¡ì…˜)
    """
    def __init__(self, celeba_dir, mpii_dir, size):
        self.size = size

        # 1) ì–¼êµ´ íŒŒì¼ ë¦¬ìŠ¤íŠ¸
        self.face_paths = glob.glob(os.path.join(celeba_dir, "*.jpg"))
        assert len(self.face_paths) > 0, f"No jpg in {celeba_dir}"

        # 2) MPII ì´ë¯¸ì§€+ìº¡ì…˜ ë¡œë“œ (ë¡œì»¬ JSON)
        print("Loading MPII dataset...")
        annotations_file = os.path.join(mpii_dir, "mpii_annotations.json")
        images_dir = os.path.join(mpii_dir, "mpii_images")
        
        with open(annotations_file, 'r') as f:
            annotations = json.load(f)
        
        self.mpii_data = []
        for annotation in annotations["image_annotations"]:
            if "image" in annotation:  # ì´ë¯¸ì§€ í•„ë“œê°€ ìˆëŠ” í•­ëª©ë§Œ ì²˜ë¦¬
                image_path = os.path.join(images_dir, annotation["image"])
                if os.path.exists(image_path):
                    self.mpii_data.append({
                        "image_path": image_path,
                        "description": annotation["description"]
                    })
        
        assert len(self.mpii_data) > 0, f"No valid MPII images found in {images_dir}"
        print(f"Loaded {len(self.mpii_data)} MPII samples")

        # 3) ê³µí†µ transform
        self.tf = transforms.Compose([
            transforms.Resize(size, Image.BILINEAR),
            transforms.CenterCrop(size),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])   # [-1,1]
        ])

    def __len__(self):
        return len(self.mpii_data)

    def __getitem__(self, idx):
        # (a) ì‚¬ëŒ ì´ë¯¸ì§€ & í…ìŠ¤íŠ¸
        sample = self.mpii_data[idx]
        image_path = sample["image_path"]
        prompt = sample["description"]
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        orig_pil = Image.open(image_path).convert("RGB")

        # (b) ê°™ì€ ë°°ì¹˜ ì•ˆì—ì„œ ì–¼êµ´ì€ **ì•„ë¬´ ì´ë¯¸ì§€ í•˜ë‚˜ ëœë¤** ì„ íƒ
        face_path = random.choice(self.face_paths)
        ref_pil = Image.open(face_path).convert("RGB")

        return {
            "ref": self.tf(ref_pil),
            "orig": self.tf(orig_pil),
            "prompt": prompt
        }
        
class FaceTextDataset(Dataset):
    def __init__(self, root, size):
        self.samples = []
        self.size = size
        for pid in sorted(os.listdir(root)):
            folder = os.path.join(root, pid)
            self.samples.append(
                dict(
                    ref=os.path.join(folder, "ref_face.jpg"),
                    orig=os.path.join(folder, "orig.jpg"),
                    prompt=open(os.path.join(folder, "prompt.txt")).read().strip()
                )
            )
        self.img_tf = transforms.Compose([
            transforms.Resize(size, Image.BILINEAR),
            transforms.CenterCrop(size),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])  # [-1,1] ë²”ìœ„
        ])

    def __len__(self): return len(self.samples)

    def __getitem__(self, idx):
        s = self.samples[idx]
        return {
            "ref"   : self.img_tf(Image.open(s["ref"]).convert("RGB")),
            "orig"  : self.img_tf(Image.open(s["orig"]).convert("RGB")),
            "prompt": s["prompt"]
        }

class InferenceDataset(Dataset):
    """
    Inferenceìš© ë°ì´í„°ì…‹ 
    ê° ìƒ˜í”Œì€ ì°¸ì¡° ì–¼êµ´, íƒ€ê²Ÿ ì´ë¯¸ì§€, í”„ë¡¬í”„íŠ¸ë¡œ êµ¬ì„±
    """
    def __init__(self, root, size):
        self.samples = []
        self.size = size
        
        for sample_id in sorted(os.listdir(root)):
            folder = os.path.join(root, sample_id)
            if os.path.isdir(folder):
                sample = {
                    "ref_face": os.path.join(folder, "ref_face.jpg"),
                    "target_img": os.path.join(folder, "target_img.jpg"), 
                    "prompt": open(os.path.join(folder, "prompt.txt")).read().strip(),
                    "sample_id": sample_id
                }
                # ëª¨ë“  í•„ìˆ˜ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                if all(os.path.exists(sample[key]) for key in ["ref_face", "target_img"]):
                    self.samples.append(sample)
        
        self.img_tf = transforms.Compose([
            transforms.Resize(size, Image.BILINEAR),
            transforms.CenterCrop(size),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])  # [-1,1] ë²”ìœ„
        ])

    def __len__(self): 
        return len(self.samples)

    def __getitem__(self, idx):
        s = self.samples[idx]
        return {
            "ref_face": self.img_tf(Image.open(s["ref_face"]).convert("RGB")),
            "target_img": self.img_tf(Image.open(s["target_img"]).convert("RGB")),
            "prompt": s["prompt"],
            "sample_id": s["sample_id"]
        }

# ----------------------------
# Face Cropping ìœ í‹¸ë¦¬í‹°
# ----------------------------
def crop_face_batch(images, mtcnn_detector, target_size=160):
    """
    ë°°ì¹˜ ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ì„ detectioní•˜ê³  cropí•©ë‹ˆë‹¤.
    Args:
        images: torch.Tensor (B, C, H, W), ê°’ ë²”ìœ„ [0, 1]
        mtcnn_detector: MTCNN ëª¨ë¸
        target_size: cropëœ ì–¼êµ´ì˜ í¬ê¸°
    Returns:
        cropped_faces: torch.Tensor (B, C, target_size, target_size)
    """
    batch_size = images.shape[0]
    device = images.device
    cropped_faces = []
    
    with torch.no_grad():  # gradient flow ì°¨ë‹¨
        for i in range(batch_size):
            # tensorë¥¼ PIL Imageë¡œ ë³€í™˜ (0-255 ë²”ìœ„)
            img_pil = transforms.ToPILImage()(images[i].cpu())
            
            # ì–¼êµ´ detection
            boxes, _ = mtcnn_detector.detect(img_pil)
            
            if boxes is not None and len(boxes) > 0:
                # ì²« ë²ˆì§¸(ê°€ì¥ í°) ì–¼êµ´ ë°•ìŠ¤ ì‚¬ìš©
                box = boxes[0]
                x1, y1, x2, y2 = [int(coord) for coord in box]
                
                # ì–¼êµ´ ì˜ì—­ crop
                face_crop = img_pil.crop((x1, y1, x2, y2))
                face_crop = face_crop.resize((target_size, target_size), Image.BILINEAR)
                
                # tensorë¡œ ë³€í™˜í•˜ì—¬ [0, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
                face_tensor = transforms.ToTensor()(face_crop)
            else:
                # ì–¼êµ´ì´ detectionë˜ì§€ ì•Šìœ¼ë©´ center crop ì‚¬ìš©
                center_crop = transforms.Compose([
                    transforms.ToPILImage(),
                    transforms.CenterCrop(min(img_pil.size)),
                    transforms.Resize((target_size, target_size)),
                    transforms.ToTensor()
                ])
                face_tensor = center_crop(images[i].cpu())
            
            cropped_faces.append(face_tensor)
    
    return torch.stack(cropped_faces).to(device)

# ----------------------------
# 4) Inference í•¨ìˆ˜
# ----------------------------
def run_inference(args):
    """
    Inference ëª¨ë“œ: í•™ìŠµëœ LoRA ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ìƒì„±
    """
    print("ğŸš€ Starting inference mode...")
    
    if args.lora_path is None:
        raise ValueError("--lora_path must be specified for inference mode")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Stable Diffusion íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
        args.pretrained_model, 
        torch_dtype=torch.float16,
        safety_checker=None
    )
    pipe.enable_vae_tiling()
    
    # UNetì€ ê¸°ë³¸ 4ì±„ë„ ìœ ì§€ (batch concatenation ì‚¬ìš©)
    print("ğŸ”§ Using batch concatenation for reference image integration...")
    
    # LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ
    print(f"ğŸ“¦ Loading LoRA weights from {args.lora_path}")
    pipe.unet.load_adapter(args.lora_path)
    pipe = pipe.to("cuda")
    
    # Inference ë°ì´í„°ì…‹ ë¡œë“œ
    print(f"ğŸ“ Loading inference dataset from {args.data_root}")
    dataset = InferenceDataset(args.data_root, args.resolution)
    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2)
    
    print(f"ğŸ¯ Found {len(dataset)} samples for inference")
    
    # ì»¤ìŠ¤í…€ inference ë£¨í”„ (reference image í†µí•©)
    with torch.no_grad():
        for i, batch in enumerate(loader):
            ref_face = batch["ref_face"][0]      # (C, H, W)
            target_img = batch["target_img"][0]  # (C, H, W)
            prompt = batch["prompt"][0]
            sample_id = batch["sample_id"][0]
            
            print(f"ğŸ–¼ï¸  Processing sample {i+1}/{len(dataset)}: {sample_id}")
            print(f"    Prompt: {prompt}")
            
            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            ref_batch = ref_face.unsqueeze(0).to("cuda")    # (1, C, H, W)
            target_batch = target_img.unsqueeze(0).to("cuda")  # (1, C, H, W)
            
            # latent spaceë¡œ ì¸ì½”ë”©
            target_latents = pipe.vae.encode(target_batch.half()).latent_dist.sample()
            target_latents = target_latents * pipe.vae.config.scaling_factor
            
            ref_latents = pipe.vae.encode(ref_batch.half()).latent_dist.sample()
            ref_latents = ref_latents * pipe.vae.config.scaling_factor
            
            # text embeddings
            text_inputs = pipe.tokenizer(
                [prompt], padding="max_length",
                max_length=pipe.tokenizer.model_max_length,
                truncation=True, return_tensors="pt"
            ).to("cuda")
            text_embeddings = pipe.text_encoder(**text_inputs)[0]
            
            # Reference imageë¥¼ CLIPìœ¼ë¡œ ì¸ì½”ë”©í•˜ì—¬ text embeddingì— ì¶”ê°€
            ref_clip_inputs = clip_processor(
                images=[transforms.ToPILImage()(ref_face * 0.5 + 0.5)],
                return_tensors="pt",
                padding=True
            ).to("cuda")
            
            ref_img_features = clip_model.get_image_features(ref_clip_inputs['pixel_values'])
            ref_img_features = F.normalize(ref_img_features, dim=-1)
            ref_img_features_expanded = ref_img_features.unsqueeze(1)  # (1, 1, 768)
            enhanced_text_embeddings = torch.cat([text_embeddings, ref_img_features_expanded], dim=1)  # (1, 78, 768)
            
            # unconditioned embeddings for classifier-free guidance
            uncond_inputs = pipe.tokenizer(
                [""], padding="max_length",
                max_length=pipe.tokenizer.model_max_length,
                truncation=True, return_tensors="pt"
            ).to("cuda")
            uncond_embeddings = pipe.text_encoder(**uncond_inputs)[0]
            
            # uncondë„ reference features ì¶”ê°€ (ë¹ˆ referenceë¡œ)
            empty_ref_features = torch.zeros_like(ref_img_features_expanded)
            enhanced_uncond_embeddings = torch.cat([uncond_embeddings, empty_ref_features], dim=1)
            
            # guidanceë¥¼ ìœ„í•´ concatenate
            text_embeddings = torch.cat([enhanced_uncond_embeddings, enhanced_text_embeddings])
            
            # noise scheduler ì´ˆê¸°í™”
            pipe.scheduler.set_timesteps(args.num_inference_steps)
            timesteps = pipe.scheduler.timesteps
            
            # ì´ˆê¸° noiseë¡œ ì‹œì‘ (img2imgì´ë¯€ë¡œ ë¶€ë¶„ì ìœ¼ë¡œ noise)
            noise = torch.randn_like(target_latents)
            init_timestep = min(int(args.num_inference_steps * args.strength), args.num_inference_steps)
            t_start = max(args.num_inference_steps - init_timestep, 0)
            
            # target latentì— noise ì¶”ê°€
            latents = pipe.scheduler.add_noise(target_latents, noise, timesteps[t_start:t_start+1])
            
            # denoising loop
            for i, t in enumerate(timesteps[t_start:]):
                # latent ì…ë ¥ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì  - reference ì—†ì´ targetë§Œ)
                latent_model_input = latents
                
                # classifier-free guidanceë¥¼ ìœ„í•´ ë³µì œ
                latent_model_input = torch.cat([latent_model_input] * 2)
                latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)
                
                # UNetìœ¼ë¡œ noise prediction (enhanced text embeddings ì‚¬ìš©)
                noise_pred = pipe.unet(
                    latent_model_input,
                    t,
                    encoder_hidden_states=text_embeddings
                ).sample
                
                # classifier-free guidance
                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                noise_pred = noise_pred_uncond + args.guidance_scale * (noise_pred_text - noise_pred_uncond)
                
                # ë‹¤ìŒ stepì„ ìœ„í•œ latent ì—…ë°ì´íŠ¸
                latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample
            
            # latentì„ ì´ë¯¸ì§€ë¡œ ë””ì½”ë”©
            generated_image = pipe.vae.decode((latents / pipe.vae.config.scaling_factor).half()).sample
            generated_image = torch.clamp(generated_image, -1, 1)
            
            # [-1,1] ë²”ìœ„ë¥¼ [0,1] ë²”ìœ„ë¡œ ë³€í™˜ í›„ PIL Imageë¡œ ë³€í™˜
            generated_pil = transforms.ToPILImage()(generated_image[0] * 0.5 + 0.5)
            target_pil = transforms.ToPILImage()(target_img * 0.5 + 0.5)
            ref_face_pil = transforms.ToPILImage()(ref_face * 0.5 + 0.5)
            
            # ê²°ê³¼ ì €ì¥
            output_path = os.path.join(args.output_dir, f"{sample_id}_generated.jpg")
            generated_pil.save(output_path)
            
            # ë¹„êµìš© ì›ë³¸ ì´ë¯¸ì§€ë“¤ë„ ì €ì¥
            ref_face_pil.save(os.path.join(args.output_dir, f"{sample_id}_ref_face.jpg"))
            target_pil.save(os.path.join(args.output_dir, f"{sample_id}_target.jpg"))
            
            print(f"    âœ… Saved: {output_path}")
    
    print(f"ğŸ‰ Inference completed! Results saved in {args.output_dir}")

# ----------------------------
# 5) ë©”ì¸ í•™ìŠµ ë£¨í”„
# ----------------------------
def train_main(args):
    accelerator = Accelerator(gradient_accumulation_steps=1)

    # (a) Stable Diffusion img2img íŒŒì´í”„ êµ¬ì„±
    pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
        args.pretrained_model, torch_dtype=torch.float16
    )
    pipe.enable_vae_tiling()  # í° í•´ìƒë„ ëŒ€ì‘
    pipe.safety_checker = None   # ê°œì¸ ì—°êµ¬ ìš©ë„

    # LoRA ì£¼ì… (UNet cross-attention ë§Œ)
    unet = pipe.unet
    
    # UNetì€ ê¸°ë³¸ 4ì±„ë„ ìœ ì§€ (batch concatenation ì‚¬ìš©)
    print("ğŸ”§ Using batch concatenation for reference image integration...")
    
    # KL Loss ê³„ì‚°ì„ ìœ„í•´ ì›ë³¸ UNet ë³µì‚¬
    unet_original = copy.deepcopy(unet) 
    unet_original = unet_original.to(accelerator.device) # ì›ë³¸ UNetì„ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    for p in unet_original.parameters(): p.requires_grad_(False) # ì›ë³¸ UNetì€ freeze
    
    lora_config = LoraConfig(
        r=8, lora_alpha=16, target_modules=["to_q", "to_k", "to_v", "to_out.0"],
        bias="none", modules_to_save=None
    )
    unet_lora = get_peft_model(unet, lora_config)
    unet_lora.print_trainable_parameters()

    # (b) CLIP - í…ìŠ¤íŠ¸/ì´ë¯¸ì§€
    clip_model      = CLIPModel.from_pretrained("openai/clip-vit-large-patch14")
    clip_processor  = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
    for p in clip_model.parameters(): p.requires_grad_(False)

    # (c) CLIP-ReID / ì–¼êµ´ ì„ë² ë”©
    # torchreid ëª¨ë¸ ì´ˆê¸°í™” (OSNet)
    reid_model = torchreid.models.build_model(
        name="osnet_x1_0",
        num_classes=1000,  # ì„ì‹œ í´ë˜ìŠ¤ ìˆ˜ (feature extractionë§Œ ì‚¬ìš©)
        loss="softmax",
        pretrained=True
    )
    reid_model = reid_model.to(accelerator.device)
    reid_model.eval()  # feature extraction ëª¨ë“œ
    
    # reid ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ frozen ìƒíƒœë¡œ ì„¤ì •
    for param in reid_model.parameters():
        param.requires_grad = False
    
    # feature extraction í•¨ìˆ˜ ì •ì˜
    def extract_reid_features(images):
        """
        torchreid ëª¨ë¸ì„ ì‚¬ìš©í•œ feature extraction
        Args:
            images: torch.Tensor (B, C, H, W), ê°’ ë²”ìœ„ [-1, 1]
        Returns:
            features: torch.Tensor (B, feature_dim)
        """
        with torch.no_grad():
            # [-1,1] ë²”ìœ„ë¥¼ [0,1] ë²”ìœ„ë¡œ ë³€í™˜
            images_norm = images * 0.5 + 0.5
            
            # torchreidëŠ” ImageNet ì •ê·œí™”ë¥¼ ì‚¬ìš©
            normalize = transforms.Normalize(
                mean=[0.485, 0.456, 0.406], 
                std=[0.229, 0.224, 0.225]
            )
            # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (torchreid ê¸°ë³¸: 256x128)
            resize_transform = transforms.Compose([
                transforms.Resize((256, 128), antialias=True),
                normalize
            ])
            
            batch_size = images.shape[0]
            features = []
            
            for i in range(batch_size):
                img = resize_transform(images_norm[i])
                img = img.unsqueeze(0).to(images.device)  # (1, C, H, W)
                
                # feature extraction
                feat = reid_model(img)
                if isinstance(feat, tuple):
                    feat = feat[0]  # ì²« ë²ˆì§¸ outputë§Œ ì‚¬ìš©
                features.append(feat.squeeze(0))
            
            return torch.stack(features)

    # (d) MTCNN face detector ì´ˆê¸°í™” (frozen)
    mtcnn_detector = MTCNN(
        image_size=160, margin=0, min_face_size=20,
        thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=False,
        device=accelerator.device, keep_all=False
    )
    # MTCNN íŒŒë¼ë¯¸í„°ë“¤ì„ frozen ìƒíƒœë¡œ ì„¤ì •
    for param in mtcnn_detector.parameters():
        param.requires_grad = False

    # (e) ì˜µí‹°ë§ˆì´ì € (LoRA íŒŒë¼ë¯¸í„°ë§Œ)
    optimizer = torch.optim.AdamW(unet_lora.parameters(), lr=args.lr)

    # (f) ë°ì´í„°
    dataset  = MixedFacePoseDataset(args.celeba_dir, args.mpii_dir, args.resolution)
    loader   = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=4)
    
    # accelerator.prepareë¡œ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ë¥¼ ì¤€ë¹„
    unet_lora, clip_model, optimizer, loader = accelerator.prepare(
        unet_lora, clip_model, optimizer, loader
    )
    
    # VAE, text_encoder, tokenizerë„ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    pipe.vae = pipe.vae.to(accelerator.device)
    pipe.text_encoder = pipe.text_encoder.to(accelerator.device)
    
    # ë””ë°”ì´ìŠ¤ í™•ì¸ ë¡œê·¸
    print(f"ğŸ–¥ï¸  Device info:")
    print(f"   Accelerator device: {accelerator.device}")
    print(f"   UNet device: {next(unet_lora.parameters()).device}")
    print(f"   VAE device: {next(pipe.vae.parameters()).device}")
    print(f"   CLIP device: {next(clip_model.parameters()).device}")
    print(f"   ReID device: {next(reid_model.parameters()).device}")

    # ìŠ¤ì¼€ì¤„ëŸ¬ â€“ SD í›ˆë ¨ê³¼ ë™ì¼(DDPM)
    noise_scheduler = DDPMScheduler(
        num_train_timesteps=1000, beta_schedule="squaredcos_cap_v2"
    )

    # ë¡œê·¸ íŒŒì¼ ì„¤ì •
    if accelerator.is_main_process:
        log_dir = os.path.join(args.save_dir, "logs")
        os.makedirs(log_dir, exist_ok=True)
        log_file = os.path.join(log_dir, f"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
        
        # CSV í—¤ë” ì‘ì„±
        with open(log_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['timestamp', 'epoch', 'global_step', 'loss_face', 'loss_text', 'loss_kl', 'total_loss'])
        
        print(f"ğŸ“ Training log will be saved to: {log_file}")

    global_step = 0
    for epoch in tqdm(range(args.epochs), desc="Epochs", position=0):
        unet_lora.train()
        
        # Create progress bar for batches
        batch_pbar = tqdm(loader, 
                         desc=f"Epoch {epoch+1}/{args.epochs}", 
                         position=1, 
                         leave=False)
        
        for batch in batch_pbar:
            with accelerator.accumulate(unet_lora):
                orig = batch["orig"]      # (B,3,H,W)   [-1,1]
                ref  = batch["ref"]       # (B,3,H,W)
                prompts = batch["prompt"]
                
                # ë°ì´í„°ë¥¼ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                orig = orig.to(accelerator.device)
                ref = ref.to(accelerator.device)

                # ---------- âŠ SD forward/backward ----------
                # encode ì›ë³¸ â†’ latent (VAEëŠ” float16ì´ë¯€ë¡œ ì…ë ¥ë„ ë§ì¶°ì¤Œ)
                latents = pipe.vae.encode(orig.half()).latent_dist.sample()
                latents = latents * pipe.vae.config.scaling_factor
                
                # encode reference image â†’ latent
                ref_latents = pipe.vae.encode(ref.half()).latent_dist.sample()
                ref_latents = ref_latents * pipe.vae.config.scaling_factor

                bsz = latents.shape[0]
                t = torch.randint(0, noise_scheduler.num_train_timesteps,
                                  (bsz,), device=latents.device).long()
                noise = torch.randn_like(latents)
                ref_noise = torch.randn_like(ref_latents)
                
                # ë‘˜ ë‹¤ ê°™ì€ timestepìœ¼ë¡œ noise ì¶”ê°€
                noisy_latents = noise_scheduler.add_noise(latents, noise, t)
                noisy_ref_latents = noise_scheduler.add_noise(ref_latents, ref_noise, t)
                
                # Reference imageë¥¼ CLIPìœ¼ë¡œ ì¸ì½”ë”©í•˜ì—¬ text embeddingì— ì¶”ê°€
                # ë¨¼ì € reference imageë¥¼ PILë¡œ ë³€í™˜
                ref_images_pil = []
                for ref_img in ref:
                    ref_pil = transforms.ToPILImage()(ref_img * 0.5 + 0.5)
                    ref_images_pil.append(ref_pil)
                
                # CLIPìœ¼ë¡œ reference image ì¸ì½”ë”©
                ref_clip_inputs = clip_processor(
                    images=ref_images_pil,
                    return_tensors="pt",
                    padding=True
                ).to(accelerator.device)
                
                ref_img_features = clip_model.get_image_features(ref_clip_inputs['pixel_values'])
                ref_img_features = F.normalize(ref_img_features, dim=-1)
                
                # text embeddings
                text_inputs = pipe.tokenizer(
                    list(prompts), 
                    padding="max_length",
                    max_length=pipe.tokenizer.model_max_length,
                    truncation=True, return_tensors="pt"
                ).to(latents.device)
                text_embeddings = pipe.text_encoder(**text_inputs)[0]
                
                # Reference image featuresë¥¼ text embeddingì— concatenate
                # text_embeddings: (B, 77, 768), ref_img_features: (B, 768)
                ref_img_features_expanded = ref_img_features.unsqueeze(1)  # (B, 1, 768)
                enhanced_text_embeddings = torch.cat([text_embeddings, ref_img_features_expanded], dim=1)  # (B, 78, 768)
                
                # ë°ì´í„° íƒ€ì…ì„ UNetê³¼ ë§ì¶¤ (float16)
                enhanced_text_embeddings = enhanced_text_embeddings.half()

                # UNet forward with enhanced text embeddings (ë©”ëª¨ë¦¬ 2ë°° ì¦ê°€ ì—†ìŒ!)
                noise_pred = unet_lora(
                    noisy_latents,  # ì›ë³¸ latentsë§Œ ì‚¬ìš©
                    t,
                    encoder_hidden_states=enhanced_text_embeddings
                ).sample

                # ---------- â‹ ì™„ì „í•œ denoisingìœ¼ë¡œ x0 ìƒì„± ----------
                # ì „ì²´ denoising processë¥¼ í†µí•´ ìµœì¢… ì´ë¯¸ì§€ ìƒì„±
                with torch.no_grad():
                    # noise schedulerë¡œ ì „ì²´ denoising ìˆ˜í–‰
                    scheduler_temp = DDPMScheduler(
                        num_train_timesteps=1000, 
                        beta_schedule="squaredcos_cap_v2"
                    )
                    scheduler_temp.set_timesteps(50)  # inference steps
                    
                    # schedulerì˜ timestepsë¥¼ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                    scheduler_temp.timesteps = scheduler_temp.timesteps.to(t.device)
                    
                    # í˜„ì¬ timestepì—ì„œ ì‹œì‘
                    current_latents = noisy_latents.clone()
                    
                    # í˜„ì¬ timestepë¶€í„° 0ê¹Œì§€ denoising
                    timesteps = scheduler_temp.timesteps.to(t.device)
                    current_t_idx = torch.where(timesteps <= t[0])[0]
                    if len(current_t_idx) > 0:
                        start_idx = current_t_idx[0].item()
                    else:
                        start_idx = 0
                    
                    for step_t in timesteps[start_idx:]:
                        # UNet prediction with enhanced text embeddings
                        step_noise_pred = unet_lora(
                            current_latents,
                            step_t.unsqueeze(0).repeat(bsz).to(current_latents.device),
                            encoder_hidden_states=enhanced_text_embeddings
                        ).sample
                        
                        # scheduler step - ì›ë³¸ ì´ë¯¸ì§€ë§Œ ì—…ë°ì´íŠ¸
                        current_latents = scheduler_temp.step(
                            step_noise_pred, step_t.cpu(), current_latents
                        ).prev_sample
                    
                    # ìµœì¢… denoised latentsë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                    imgs_gen = pipe.vae.decode(
                        (current_latents / pipe.vae.config.scaling_factor).half()).sample
                imgs_gen = torch.clamp(imgs_gen, -1, 1)

                # ---------- âŒ CLIP-ReID ì–¼êµ´ ì†ì‹¤ ----------
                # ì–¼êµ´ detection & crop ì ìš©
                # ë¨¼ì € [-1,1] ë²”ìœ„ë¥¼ [0,1] ë²”ìœ„ë¡œ ë³€í™˜
                ref_imgs_norm = ref * 0.5 + 0.5  # [-1,1] â†’ [0,1]
                gen_imgs_norm = imgs_gen * 0.5 + 0.5  # [-1,1] â†’ [0,1]
                
                # ì–¼êµ´ ì˜ì—­ crop (gradient flow ì°¨ë‹¨)
                with torch.no_grad():
                    ref_faces = crop_face_batch(ref_imgs_norm, mtcnn_detector, target_size=224)
                    gen_faces = crop_face_batch(gen_imgs_norm, mtcnn_detector, target_size=224)
                
                # ReID feature extractionì„ ìœ„í•´ [0,1] â†’ [-1,1] ë²”ìœ„ë¡œ ë³€í™˜
                ref_faces_reid = ref_faces * 2.0 - 1.0  # [0,1] â†’ [-1,1]
                gen_faces_reid = gen_faces * 2.0 - 1.0  # [0,1] â†’ [-1,1]
                
                # CLIP-ReID íŠ¹ì§• ì¶”ì¶œ (cropëœ ì–¼êµ´ ì‚¬ìš©)
                ref_feats = extract_reid_features(ref_faces_reid)   # (B, 512)
                gen_feats = extract_reid_features(gen_faces_reid)   # (B, 512)
                sim_face  = F.cosine_similarity(ref_feats, gen_feats, dim=1)
                loss_face = (1 - sim_face).mean()      # ë†’ì„ìˆ˜ë¡ ì¢‹ìœ¼ë‹ˆ 1-cosine

                # ---------- â CLIP í…ìŠ¤íŠ¸ ì†ì‹¤ ----------
                # ìƒì„±ëœ ì´ë¯¸ì§€ë¥¼ PIL Image í˜•íƒœë¡œ ë³€í™˜ (CLIP processorìš©)
                gen_images_pil = []
                for g in imgs_gen:
                    img_pil = transforms.ToPILImage()(g * 0.5 + 0.5)
                    gen_images_pil.append(img_pil)
                
                clip_inputs = clip_processor(
                    text=list(prompts),
                    images=gen_images_pil,
                    return_tensors="pt",
                    padding=True
                )
                
                # CLIP ì…ë ¥ì„ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                clip_inputs = {k: v.to(accelerator.device) if isinstance(v, torch.Tensor) else v 
                             for k, v in clip_inputs.items()}

                img_emb = clip_model.get_image_features(clip_inputs['pixel_values'])
                txt_emb = clip_model.get_text_features(clip_inputs['input_ids'],
                                                       attention_mask=clip_inputs.get('attention_mask', None))
                img_emb, txt_emb = F.normalize(img_emb, dim=-1), F.normalize(txt_emb, dim=-1)
                sim_text = (img_emb * txt_emb).sum(-1)
                loss_text = (1 - sim_text).mean()

                # ---------- â ì´ ì†ì‹¤ ----------
                # KL divergence loss ê³„ì‚° (ì•ˆì •ì ì¸ ë²„ì „)
                with torch.no_grad():
                    original_output = unet_original(
                        noisy_latents,
                        t,
                        encoder_hidden_states=enhanced_text_embeddings
                    ).sample
                
                # ì•ˆì •ì ì¸ KL divergence ê³„ì‚°
                # 1) ê°’ë“¤ì„ clampí•´ì„œ ê·¹ê°’ ë°©ì§€
                noise_pred_clamped = torch.clamp(noise_pred, -10, 10)
                original_output_clamped = torch.clamp(original_output, -10, 10)
                
                # 2) temperature scalingìœ¼ë¡œ ë¶„í¬ë¥¼ ë¶€ë“œëŸ½ê²Œ ë§Œë“¦
                temperature = 2.0
                log_p = F.log_softmax(noise_pred_clamped / temperature, dim=1)
                q = F.softmax(original_output_clamped / temperature, dim=1)
                
                # 3) KL divergence ê³„ì‚° (ì•ˆì •ì ì¸ ë²„ì „)
                kl_div = F.kl_div(log_p, q, reduction='batchmean')
                
                # 4) NaN ì²´í¬ ë° ì²˜ë¦¬
                if torch.isnan(kl_div) or torch.isinf(kl_div):
                    # print(f"âš ï¸  KL divergence is {kl_div.item()}, setting to 0")
                    kl_div = torch.tensor(0.0, device=kl_div.device, requires_grad=True)
                
                # ì´ ì†ì‹¤ ê³„ì‚°
                loss = args.l_face * loss_face + args.l_text * loss_text + args.l_kl * kl_div
                accelerator.backward(loss)
                optimizer.step(); optimizer.zero_grad()

                global_step += 1
                
                # Update progress bar with current loss values
                if global_step % 10 == 0:
                    batch_pbar.set_postfix({
                        'L_face': f'{loss_face.item():.3f}',
                        'L_text': f'{loss_text.item():.3f}',
                        'L_kl': f'{kl_div.item():.3f}',
                        'total': f'{loss.item():.3f}'
                    })
                
                # 1000ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡
                if accelerator.is_main_process and global_step % 1000 == 0:
                    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    with open(log_file, 'a', newline='') as f:
                        writer = csv.writer(f)
                        writer.writerow([
                            timestamp,
                            epoch + 1,
                            global_step,
                            f"{loss_face.item():.6f}",
                            f"{loss_text.item():.6f}",
                            f"{kl_div.item():.6f}",
                            f"{loss.item():.6f}"
                        ])
                    print(f"ğŸ“ Logged training metrics at step {global_step} to {log_file}")

        # epoch-ë‹¨ìœ„ LoRA ê°€ì¤‘ì¹˜ ì €ì¥
        if accelerator.is_main_process:
            os.makedirs(args.save_dir, exist_ok=True)
            unet_lora.save_pretrained(os.path.join(args.save_dir, f"epoch{epoch:02d}"))
            print(f"âœ… Epoch {epoch+1} completed. LoRA weights saved to {args.save_dir}/epoch{epoch:02d}")

def main():
    args = parse_args()
    
    if args.mode == "train":
        print("ğŸ‹ï¸ Starting training mode...")
        train_main(args)
    elif args.mode == "inference":
        print("ğŸ¯ Starting inference mode...")
        run_inference(args)
    else:
        raise ValueError(f"Unknown mode: {args.mode}")

if __name__ == "__main__":
    main()
