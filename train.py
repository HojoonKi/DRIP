import os, math, random, argparse, torch, torch.nn.functional as F
import glob
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from torchvision import transforms
from diffusers import (
    StableDiffusionImg2ImgPipeline,
    AutoencoderKL,
    DDPMScheduler,
    UNet2DConditionModel,
)
from diffusers.loaders import AttnProcsLayers
from peft import LoraConfig, get_peft_model
from transformers import CLIPProcessor, CLIPModel
import torchreid
from accelerate import Accelerator
from facenet_pytorch import MTCNN
import numpy as np
from datasets import load_dataset

# ----------------------------
# 1) í•˜ì´í¼íŒŒë¼ë¯¸í„° & ì¸ì
# ----------------------------
def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--mode", type=str, default="train", choices=["train", "inference"],
                   help="ì‹¤í–‰ ëª¨ë“œ: train ë˜ëŠ” inference")
    p.add_argument("--data_root",   type=str, default="dataset")
    p.add_argument("--pretrained_model", type=str,
                   default="runwayml/stable-diffusion-v1-5")
    p.add_argument("--resolution", type=int, default=512)
    p.add_argument("--batch_size", type=int, default=2)
    p.add_argument("--lr",         type=float, default=1e-4)
    p.add_argument("--epochs",     type=int, default=3)
    p.add_argument("--l_face",     type=float, default=1.0)   # Î»â‚
    p.add_argument("--l_text",     type=float, default=1.0)   # Î»â‚‚
    p.add_argument("--save_dir",   type=str, default="lora_out")
    
    # Inference ê´€ë ¨ ì˜µì…˜ë“¤
    p.add_argument("--lora_path", type=str, default=None,
                   help="inferenceì‹œ ë¡œë“œí•  LoRA ê°€ì¤‘ì¹˜ ê²½ë¡œ")
    p.add_argument("--output_dir", type=str, default="inference_output",
                   help="inference ê²°ê³¼ ì €ì¥ ê²½ë¡œ")
    p.add_argument("--num_inference_steps", type=int, default=50,
                   help="inferenceì‹œ diffusion steps ìˆ˜")
    p.add_argument("--guidance_scale", type=float, default=7.5,
                   help="classifier-free guidance scale")
    p.add_argument("--strength", type=float, default=0.8,
                   help="img2img strength (0.0-1.0)")
    
    # ë°ì´í„°ì…‹ ê´€ë ¨ ì˜µì…˜ë“¤
    p.add_argument("--celeba_dir", type=str,
               default="dataset/celeba-hq/celeba-512",
               help="CelebA-HQ 256px ì–¼êµ´ ì´ë¯¸ì§€ í´ë”")
    p.add_argument("--mpii_split", type=str, default="train",
               choices=["train", "validation", "test"],
               help="MPII ë°ì´í„°ì…‹ split")
    
    return p.parse_args()

# ----------------------------
# 2) ë°ì´í„°ì…‹
# ----------------------------

class MixedFacePoseDataset(Dataset):
    """
    CelebA ì–¼êµ´ â†” MPII ì‚¬ëŒ ì´ë¯¸ì§€Â·ìº¡ì…˜ì„ ë§¤ ìŠ¤í… ëœë¤ ë§¤ì¹­
    ë°˜í™˜: ref(ì–¼êµ´), orig(ì‚¬ëŒ+ë°°ê²½), prompt(ìº¡ì…˜)
    """
    def __init__(self, celeba_dir, mp_split, size):
        self.size = size

        # 1) ì–¼êµ´ íŒŒì¼ ë¦¬ìŠ¤íŠ¸
        self.face_paths = glob.glob(os.path.join(celeba_dir, "*.jpg"))
        assert len(self.face_paths) > 0, f"No jpg in {celeba_dir}"

        # 2) MPII ì´ë¯¸ì§€+ìº¡ì…˜ ë¡œë“œ (ğŸ¤—)
        print("Loading MPII dataset...")
        ds = load_dataset("saifkhichi96/mpii-human-pose-captions", split=mp_split)
        self.mpii_data = ds  # Store the whole dataset
        self.mpii_caps = ds["description"]   # str - descriptions
        assert len(self.mpii_data) > 0

        # 3) ê³µí†µ transform
        self.tf = transforms.Compose([
            transforms.Resize(size, Image.BILINEAR),
            transforms.CenterCrop(size),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])   # [-1,1]
        ])

    def __len__(self):
        return len(self.mpii_data)

    def __getitem__(self, idx):
        # (a) ì‚¬ëŒ ì´ë¯¸ì§€ & í…ìŠ¤íŠ¸
        sample = self.mpii_data[idx]
        
        # Handle the image - it might be a PIL Image or need to be loaded
        img_data = sample["image"]
        if isinstance(img_data, str):
            # If it's a filename, we need to handle it (this shouldn't happen with this dataset)
            print(f"Warning: Got filename instead of PIL Image: {img_data}")
            # You might need to adjust this path based on where images are stored
            orig_pil = Image.open(img_data).convert("RGB")
        elif hasattr(img_data, 'convert'):
            # It's already a PIL Image
            orig_pil = img_data.convert("RGB")
        else:
            # Convert from numpy array or other format to PIL
            orig_pil = Image.fromarray(img_data).convert("RGB")
        
        prompt = sample["description"]

        # (b) ê°™ì€ ë°°ì¹˜ ì•ˆì—ì„œ ì–¼êµ´ì€ **ì•„ë¬´ ì´ë¯¸ì§€ í•˜ë‚˜ ëœë¤** ì„ íƒ
        face_path = random.choice(self.face_paths)
        ref_pil = Image.open(face_path).convert("RGB")

        return {
            "ref": self.tf(ref_pil),
            "orig": self.tf(orig_pil),
            "prompt": prompt
        }
        
class FaceTextDataset(Dataset):
    def __init__(self, root, size):
        self.samples = []
        self.size = size
        for pid in sorted(os.listdir(root)):
            folder = os.path.join(root, pid)
            self.samples.append(
                dict(
                    ref=os.path.join(folder, "ref_face.jpg"),
                    orig=os.path.join(folder, "orig.jpg"),
                    prompt=open(os.path.join(folder, "prompt.txt")).read().strip()
                )
            )
        self.img_tf = transforms.Compose([
            transforms.Resize(size, Image.BILINEAR),
            transforms.CenterCrop(size),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])  # [-1,1] ë²”ìœ„
        ])

    def __len__(self): return len(self.samples)

    def __getitem__(self, idx):
        s = self.samples[idx]
        return {
            "ref"   : self.img_tf(Image.open(s["ref"]).convert("RGB")),
            "orig"  : self.img_tf(Image.open(s["orig"]).convert("RGB")),
            "prompt": s["prompt"]
        }

class InferenceDataset(Dataset):
    """
    Inferenceìš© ë°ì´í„°ì…‹ 
    ê° ìƒ˜í”Œì€ ì°¸ì¡° ì–¼êµ´, íƒ€ê²Ÿ ì´ë¯¸ì§€, í”„ë¡¬í”„íŠ¸ë¡œ êµ¬ì„±
    """
    def __init__(self, root, size):
        self.samples = []
        self.size = size
        
        for sample_id in sorted(os.listdir(root)):
            folder = os.path.join(root, sample_id)
            if os.path.isdir(folder):
                sample = {
                    "ref_face": os.path.join(folder, "ref_face.jpg"),
                    "target_img": os.path.join(folder, "target_img.jpg"), 
                    "prompt": open(os.path.join(folder, "prompt.txt")).read().strip(),
                    "sample_id": sample_id
                }
                # ëª¨ë“  í•„ìˆ˜ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                if all(os.path.exists(sample[key]) for key in ["ref_face", "target_img"]):
                    self.samples.append(sample)
        
        self.img_tf = transforms.Compose([
            transforms.Resize(size, Image.BILINEAR),
            transforms.CenterCrop(size),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])  # [-1,1] ë²”ìœ„
        ])

    def __len__(self): 
        return len(self.samples)

    def __getitem__(self, idx):
        s = self.samples[idx]
        return {
            "ref_face": self.img_tf(Image.open(s["ref_face"]).convert("RGB")),
            "target_img": self.img_tf(Image.open(s["target_img"]).convert("RGB")),
            "prompt": s["prompt"],
            "sample_id": s["sample_id"]
        }

# ----------------------------
# Face Cropping ìœ í‹¸ë¦¬í‹°
# ----------------------------
def crop_face_batch(images, mtcnn_detector, target_size=160):
    """
    ë°°ì¹˜ ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ì„ detectioní•˜ê³  cropí•©ë‹ˆë‹¤.
    Args:
        images: torch.Tensor (B, C, H, W), ê°’ ë²”ìœ„ [0, 1]
        mtcnn_detector: MTCNN ëª¨ë¸
        target_size: cropëœ ì–¼êµ´ì˜ í¬ê¸°
    Returns:
        cropped_faces: torch.Tensor (B, C, target_size, target_size)
    """
    batch_size = images.shape[0]
    device = images.device
    cropped_faces = []
    
    with torch.no_grad():  # gradient flow ì°¨ë‹¨
        for i in range(batch_size):
            # tensorë¥¼ PIL Imageë¡œ ë³€í™˜ (0-255 ë²”ìœ„)
            img_pil = transforms.ToPILImage()(images[i].cpu())
            
            # ì–¼êµ´ detection
            boxes, _ = mtcnn_detector.detect(img_pil)
            
            if boxes is not None and len(boxes) > 0:
                # ì²« ë²ˆì§¸(ê°€ì¥ í°) ì–¼êµ´ ë°•ìŠ¤ ì‚¬ìš©
                box = boxes[0]
                x1, y1, x2, y2 = [int(coord) for coord in box]
                
                # ì–¼êµ´ ì˜ì—­ crop
                face_crop = img_pil.crop((x1, y1, x2, y2))
                face_crop = face_crop.resize((target_size, target_size), Image.BILINEAR)
                
                # tensorë¡œ ë³€í™˜í•˜ì—¬ [0, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
                face_tensor = transforms.ToTensor()(face_crop)
            else:
                # ì–¼êµ´ì´ detectionë˜ì§€ ì•Šìœ¼ë©´ center crop ì‚¬ìš©
                center_crop = transforms.Compose([
                    transforms.ToPILImage(),
                    transforms.CenterCrop(min(img_pil.size)),
                    transforms.Resize((target_size, target_size)),
                    transforms.ToTensor()
                ])
                face_tensor = center_crop(images[i].cpu())
            
            cropped_faces.append(face_tensor)
    
    return torch.stack(cropped_faces).to(device)

# ----------------------------
# 4) Inference í•¨ìˆ˜
# ----------------------------
def run_inference(args):
    """
    Inference ëª¨ë“œ: í•™ìŠµëœ LoRA ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ìƒì„±
    """
    print("ğŸš€ Starting inference mode...")
    
    if args.lora_path is None:
        raise ValueError("--lora_path must be specified for inference mode")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Stable Diffusion íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
        args.pretrained_model, 
        torch_dtype=torch.float16,
        safety_checker=None
    )
    pipe.enable_vae_tiling()
    
    # LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ
    print(f"ğŸ“¦ Loading LoRA weights from {args.lora_path}")
    pipe.unet.load_adapter(args.lora_path)
    pipe = pipe.to("cuda")
    
    # Inference ë°ì´í„°ì…‹ ë¡œë“œ
    print(f"ğŸ“ Loading inference dataset from {args.data_root}")
    dataset = InferenceDataset(args.data_root, args.resolution)
    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2)
    
    print(f"ğŸ¯ Found {len(dataset)} samples for inference")
    
    # Inference ì‹¤í–‰
    with torch.no_grad():
        for i, batch in enumerate(loader):
            ref_face = batch["ref_face"][0]      # (C, H, W)
            target_img = batch["target_img"][0]  # (C, H, W)
            prompt = batch["prompt"][0]
            sample_id = batch["sample_id"][0]
            
            print(f"ğŸ–¼ï¸  Processing sample {i+1}/{len(dataset)}: {sample_id}")
            print(f"    Prompt: {prompt}")
            
            # [-1,1] ë²”ìœ„ë¥¼ [0,1] ë²”ìœ„ë¡œ ë³€í™˜ í›„ PIL Imageë¡œ ë³€í™˜
            target_pil = transforms.ToPILImage()(target_img * 0.5 + 0.5)
            
            # img2img ìƒì„±
            generated_images = pipe(
                prompt=prompt,
                image=target_pil,
                strength=args.strength,
                num_inference_steps=args.num_inference_steps,
                guidance_scale=args.guidance_scale,
                num_images_per_prompt=1
            ).images
            
            # ê²°ê³¼ ì €ì¥
            output_path = os.path.join(args.output_dir, f"{sample_id}_generated.jpg")
            generated_images[0].save(output_path)
            
            # ë¹„êµìš© ì›ë³¸ ì´ë¯¸ì§€ë“¤ë„ ì €ì¥
            ref_face_pil = transforms.ToPILImage()(ref_face * 0.5 + 0.5)
            ref_face_pil.save(os.path.join(args.output_dir, f"{sample_id}_ref_face.jpg"))
            target_pil.save(os.path.join(args.output_dir, f"{sample_id}_target.jpg"))
            
            print(f"    âœ… Saved: {output_path}")
    
    print(f"ğŸ‰ Inference completed! Results saved in {args.output_dir}")

# ----------------------------
# 5) ë©”ì¸ í•™ìŠµ ë£¨í”„
# ----------------------------
def train_main(args):
    accelerator = Accelerator(gradient_accumulation_steps=1)

    # (a) Stable Diffusion img2img íŒŒì´í”„ êµ¬ì„±
    pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
        args.pretrained_model, torch_dtype=torch.float16
    )
    pipe.enable_vae_tiling()  # í° í•´ìƒë„ ëŒ€ì‘
    pipe.safety_checker = None   # ê°œì¸ ì—°êµ¬ ìš©ë„

    # LoRA ì£¼ì… (UNet cross-attention ë§Œ)
    unet = pipe.unet
    lora_config = LoraConfig(
        r=8, lora_alpha=16, target_modules=["to_q", "to_k", "to_v", "to_out.0"],
        bias="none", modules_to_save=None
    )
    unet_lora = get_peft_model(unet, lora_config)
    unet_lora.print_trainable_parameters()

    # (b) CLIP - í…ìŠ¤íŠ¸/ì´ë¯¸ì§€
    clip_model      = CLIPModel.from_pretrained("openai/clip-vit-large-patch14")
    clip_processor  = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
    for p in clip_model.parameters(): p.requires_grad_(False)

    # (c) CLIP-ReID / ì–¼êµ´ ì„ë² ë”©
    # torchreid ëª¨ë¸ ì´ˆê¸°í™” (OSNet)
    reid_model = torchreid.models.build_model(
        name="osnet_x1_0",
        num_classes=1000,  # ì„ì‹œ í´ë˜ìŠ¤ ìˆ˜ (feature extractionë§Œ ì‚¬ìš©)
        loss="softmax",
        pretrained=True
    )
    reid_model = reid_model.to(accelerator.device)
    reid_model.eval()  # feature extraction ëª¨ë“œ
    
    # reid ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ frozen ìƒíƒœë¡œ ì„¤ì •
    for param in reid_model.parameters():
        param.requires_grad = False
    
    # feature extraction í•¨ìˆ˜ ì •ì˜
    def extract_reid_features(images):
        """
        torchreid ëª¨ë¸ì„ ì‚¬ìš©í•œ feature extraction
        Args:
            images: torch.Tensor (B, C, H, W), ê°’ ë²”ìœ„ [0, 1]
        Returns:
            features: torch.Tensor (B, feature_dim)
        """
        with torch.no_grad():
            # torchreidëŠ” ImageNet ì •ê·œí™”ë¥¼ ì‚¬ìš©
            normalize = transforms.Normalize(
                mean=[0.485, 0.456, 0.406], 
                std=[0.229, 0.224, 0.225]
            )
            # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (torchreid ê¸°ë³¸: 256x128)
            resize_transform = transforms.Compose([
                transforms.Resize((256, 128), antialias=True),
                normalize
            ])
            
            batch_size = images.shape[0]
            features = []
            
            for i in range(batch_size):
                img = resize_transform(images[i])
                img = img.unsqueeze(0)  # (1, C, H, W)
                
                # feature extraction
                feat = reid_model(img)
                if isinstance(feat, tuple):
                    feat = feat[0]  # ì²« ë²ˆì§¸ outputë§Œ ì‚¬ìš©
                features.append(feat.squeeze(0))
            
            return torch.stack(features)

    # (d) MTCNN face detector ì´ˆê¸°í™” (frozen)
    mtcnn_detector = MTCNN(
        image_size=160, margin=0, min_face_size=20,
        thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=False,
        device=accelerator.device, keep_all=False
    )
    # MTCNN íŒŒë¼ë¯¸í„°ë“¤ì„ frozen ìƒíƒœë¡œ ì„¤ì •
    for param in mtcnn_detector.parameters():
        param.requires_grad = False

    # (e) ì˜µí‹°ë§ˆì´ì € (LoRA íŒŒë¼ë¯¸í„°ë§Œ)
    optimizer = torch.optim.AdamW(unet_lora.parameters(), lr=args.lr)

    # (f) ë°ì´í„°
    dataset  = MixedFacePoseDataset(args.celeba_dir, args.mpii_split, args.resolution)
    loader   = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=4)
    unet_lora, clip_model, optimizer, loader = accelerator.prepare(
        unet_lora, clip_model, optimizer, loader
    )

    # ìŠ¤ì¼€ì¤„ëŸ¬ â€“ SD í›ˆë ¨ê³¼ ë™ì¼(DDPM)
    noise_scheduler = DDPMScheduler(
        num_train_timesteps=1000, beta_schedule="squaredcos_cap_v2"
    )

    global_step = 0
    for epoch in range(args.epochs):
        unet_lora.train()
        for batch in loader:
            with accelerator.accumulate(unet_lora):
                orig = batch["orig"]      # (B,3,H,W)   [-1,1]
                ref  = batch["ref"]       # (B,3,H,W)
                prompts = batch["prompt"]

                # ---------- âŠ SD forward/backward ----------
                # encode ì›ë³¸ â†’ latent
                latents = pipe.vae.encode(orig).latent_dist.sample()
                latents = latents * pipe.vae.config.scaling_factor

                bsz = latents.shape[0]
                t = torch.randint(0, noise_scheduler.num_train_timesteps,
                                  (bsz,), device=latents.device).long()
                noise = torch.randn_like(latents)
                noisy_latents = noise_scheduler.add_noise(latents, noise, t)

                # text embeddings
                text_inputs = pipe.tokenizer(
                    list(prompts), padding="max_length",
                    max_length=pipe.tokenizer.model_max_length,
                    truncation=True, return_tensors="pt"
                ).to(latents.device)
                text_embeddings = pipe.text_encoder(**text_inputs)[0]

                # UNet forward
                noise_pred = unet_lora(
                    noisy_latents,
                    t,
                    encoder_hidden_states=text_embeddings
                ).sample

                # simple MSE loss between pred & true noise (optional, small weight)
                loss_recon = F.mse_loss(noise_pred.float(), noise.float())

                # ---------- â‹ ì´ë¯¸ì§€ ì¬ìƒì„±(í•œ ìŠ¤í…ë§Œ ë°˜ì „) ----------
                with torch.no_grad():
                    denoised_latents = noisy_latents - noise_pred
                    imgs_gen = pipe.vae.decode(
                        denoised_latents / pipe.vae.config.scaling_factor).sample
                imgs_gen = torch.clamp(imgs_gen, -1, 1)

                # ---------- âŒ CLIP-ReID ì–¼êµ´ ì†ì‹¤ ----------
                # ì–¼êµ´ detection & crop ì ìš©
                # ë¨¼ì € [-1,1] ë²”ìœ„ë¥¼ [0,1] ë²”ìœ„ë¡œ ë³€í™˜
                ref_imgs_norm = ref * 0.5 + 0.5  # [-1,1] â†’ [0,1]
                gen_imgs_norm = imgs_gen * 0.5 + 0.5  # [-1,1] â†’ [0,1]
                
                # ì–¼êµ´ ì˜ì—­ crop (gradient flow ì°¨ë‹¨)
                with torch.no_grad():
                    ref_faces = crop_face_batch(ref_imgs_norm, mtcnn_detector, target_size=224)
                    gen_faces = crop_face_batch(gen_imgs_norm, mtcnn_detector, target_size=224)
                
                # ReID feature extractionì„ ìœ„í•´ ë‹¤ì‹œ [-1,1] ë²”ìœ„ë¡œ ë³€í™˜
                ref_faces_reid = ref_faces * 2.0 - 1.0  # [0,1] â†’ [-1,1]
                gen_faces_reid = gen_faces * 2.0 - 1.0  # [0,1] â†’ [-1,1]
                
                # CLIP-ReID íŠ¹ì§• ì¶”ì¶œ (cropëœ ì–¼êµ´ ì‚¬ìš©)
                ref_feats = extract_reid_features(ref_faces_reid.half())   # (B, 512)
                gen_feats = extract_reid_features(gen_faces_reid.half())   # (B, 512)
                sim_face  = F.cosine_similarity(ref_feats, gen_feats, dim=1)
                loss_face = (1 - sim_face).mean()      # ë†’ì„ìˆ˜ë¡ ì¢‹ìœ¼ë‹ˆ 1-cosine

                # ---------- â CLIP í…ìŠ¤íŠ¸ ì†ì‹¤ ----------
                clip_inputs = clip_processor(
                    text=list(prompts),
                    images=[(g * 0.5 + 0.5) for g in imgs_gen],
                    return_tensors="pt",
                    padding=True
                ).to(latents.device)

                img_emb = clip_model.get_image_features(clip_inputs.pixel_values)
                txt_emb = clip_model.get_text_features(clip_inputs.input_ids,
                                                       attention_mask=clip_inputs.attention_mask)
                img_emb, txt_emb = F.normalize(img_emb, dim=-1), F.normalize(txt_emb, dim=-1)
                sim_text = (img_emb * txt_emb).sum(-1)
                loss_text = (1 - sim_text).mean()

                # ---------- â ì´ ì†ì‹¤ ----------
                loss = args.l_face * loss_face + args.l_text * loss_text + 0.1 * loss_recon
                accelerator.backward(loss)
                optimizer.step(); optimizer.zero_grad()

                global_step += 1
                if accelerator.is_main_process and global_step % 50 == 0:
                    print(f"step {global_step:05d} | "
                          f"L_face {loss_face.item():.3f} "
                          f"L_text {loss_text.item():.3f}")

        # epoch-ë‹¨ìœ„ LoRA ê°€ì¤‘ì¹˜ ì €ì¥
        if accelerator.is_main_process:
            os.makedirs(args.save_dir, exist_ok=True)
            unet_lora.save_pretrained(os.path.join(args.save_dir, f"epoch{epoch:02d}"))

def main():
    args = parse_args()
    
    if args.mode == "train":
        print("ğŸ‹ï¸ Starting training mode...")
        train_main(args)
    elif args.mode == "inference":
        print("ğŸ¯ Starting inference mode...")
        run_inference(args)
    else:
        raise ValueError(f"Unknown mode: {args.mode}")

if __name__ == "__main__":
    main()
